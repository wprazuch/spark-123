{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/12 15:08:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Playground\")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/FL_insurance_sample.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([1,2,3,4,5]).collect()\n",
    "\n",
    "def rdd_from_list(sc, n):\n",
    "    return sc.parallelize(range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(rdd_from_list(sc, 10).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = rdd_from_list(sc, 3)\n",
    "assert isinstance(result_rdd, RDD)\n",
    "assert result_rdd.collect() == [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_to_rdd(sc, path):\n",
    "    return sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = load_file_to_rdd(sc, filepath)\n",
    "\n",
    "assert isinstance(result_rdd, RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(spark, rdd, schema):\n",
    "    \"\"\"Creates a dataframe from an RDD and a schema\"\"\"\n",
    "    return spark.createDataFrame(rdd, schema)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([('1', 'a'), ('2', 'b'), ('3', 'c'), ('4', 'd'), ('5', 'e'), ('6', 'f')])\n",
    "schema = StructType([StructField('id', StringType(), True), StructField('letter', StringType(), True)])\n",
    "\n",
    "result_df = create_dataframe(spark, rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assert result_df.schema == schema\n",
    "assert result_df.rdd.collect() == rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders_rdd = spark.sparkContext.parallelize([('1', 'M'), ('2', 'F'), ('3', 'F'), ('4', 'M'), ('5', 'F'), ('6', 'M')])\n",
    "grades_rdd = spark.sparkContext.parallelize([('1', 1.0), ('2', 2.0), ('3', 3.0), ('4', 4.0), ('5', 5.0), ('6', 6.0)])\n",
    "\n",
    "genders_schema = StructType([StructField('ID', StringType(), True), StructField('gender', StringType(), True)])\n",
    "grades_schema = StructType([StructField('ID', StringType(), True), StructField('grade', FloatType(), True)])\n",
    "\n",
    "genders_df = create_dataframe(spark, genders_rdd, genders_schema)\n",
    "grades_df = create_dataframe(spark, grades_rdd, grades_schema)\n",
    "\n",
    "genders_df.createOrReplaceTempView('genders')\n",
    "grades_df.createOrReplaceTempView('grades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, gender: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genders_df.filter(genders_df['ID'] > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| ID|gender|\n",
      "+---+------+\n",
      "|  3|     F|\n",
      "|  4|     M|\n",
      "|  5|     F|\n",
      "|  6|     M|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genders_df[genders_df['ID'] > 2].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| ID|gender|\n",
      "+---+------+\n",
      "|  3|     F|\n",
      "|  4|     M|\n",
      "|  5|     F|\n",
      "|  6|     M|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM genders WHERE ID > 2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| ID|gender|\n",
      "+---+------+\n",
      "|  3|     F|\n",
      "|  4|     M|\n",
      "|  5|     F|\n",
      "|  6|     M|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genders_df[genders_df['ID'] > 2].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([[1, 3], [2, 9]]).map(lambda row: row[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op1(sc, matrix):\n",
    "    matrix = sc.parallelize(matrix).map(lambda row: row[0]*2).collect()\n",
    "    matrix = sc.parallelize(matrix).map(lambda row: row[1]-3).collect()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RDD' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m matrix \u001b[39m=\u001b[39m [[\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m], [\u001b[39m2\u001b[39m,\u001b[39m5\u001b[39m], [\u001b[39m8\u001b[39m,\u001b[39m9\u001b[39m]]\n\u001b[1;32m      2\u001b[0m matrix_rdd \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mparallelize(matrix)\n\u001b[0;32m----> 3\u001b[0m result_rdd \u001b[39m=\u001b[39m op1(sc, matrix_rdd)\n\u001b[1;32m      5\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(result_rdd, RDD)\n\u001b[1;32m      6\u001b[0m \u001b[39massert\u001b[39;00m result_rdd\u001b[39m.\u001b[39mcollect() \u001b[39m==\u001b[39m [[\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m], [\u001b[39m4\u001b[39m, \u001b[39m2\u001b[39m], [\u001b[39m16\u001b[39m, \u001b[39m6\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m, in \u001b[0;36mop1\u001b[0;34m(sc, matrix)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mop1\u001b[39m(sc, matrix):\n\u001b[0;32m----> 2\u001b[0m     matrix \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mparallelize(matrix)\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m row: row[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m      3\u001b[0m     matrix \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mparallelize(matrix)\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m row: row[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcollect()\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m matrix\n",
      "File \u001b[0;32m~/Projects/Personal/spark-123/.venv/lib/python3.10/site-packages/pyspark/context.py:807\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[39m# Make sure we distribute data evenly if it's smaller than self.batchSize\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m__len__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m(c):\n\u001b[0;32m--> 807\u001b[0m     c \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(c)  \u001b[39m# Make it a list so we can compute its length\u001b[39;00m\n\u001b[1;32m    808\u001b[0m batchSize \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\n\u001b[1;32m    809\u001b[0m     \u001b[39m1\u001b[39m, \u001b[39mmin\u001b[39m(\u001b[39mlen\u001b[39m(c) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m numSlices, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batchSize \u001b[39mor\u001b[39;00m \u001b[39m1024\u001b[39m)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    810\u001b[0m )\n\u001b[1;32m    811\u001b[0m serializer \u001b[39m=\u001b[39m BatchedSerializer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbatched_serializer, batchSize)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'RDD' object is not iterable"
     ]
    }
   ],
   "source": [
    "matrix = [[1,3], [2,5], [8,9]]\n",
    "matrix_rdd = sc.parallelize(matrix)\n",
    "result_rdd = op1(sc, matrix_rdd)\n",
    "\n",
    "assert isinstance(result_rdd, RDD)\n",
    "assert result_rdd.collect() == [[2, 0], [4, 2], [16, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
